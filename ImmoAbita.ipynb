{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import urllib3\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "url='https://www.immoabita.be/en/List/Page/16?pageId=0&sort=None'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "#options.add_argument('user-data-dir=chrome_dir_final')\n",
    "#driver = webdriver.Chrome(options=options,executable_path= '/home/becode/anaconda3/bin/chromedriver')\n",
    "driver.implicitly_wait(30)\n",
    "#driver.get(url)\n",
    "\n",
    "soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#Get web links of all properties\n",
    "def get_property_links(url, driver, pages=20):\n",
    "    prop_links=[]\n",
    "    driver.get(url)\n",
    "    for i in range(pages):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        listings = soup.find_all(\"a\", class_=\"estate-card\")\n",
    "        for row in listings:\n",
    "            if row.has_attr(\"href\"):\n",
    "                page_data = 'https://www.immoabita.be'+row['href']\n",
    "            prop_links.append(page_data)\n",
    "        time.sleep(np.random.lognormal(0,1))\n",
    "        next_button = soup.find_all(\"a\", class_=\"list-pagging__link\")\n",
    "        for row in next_button:\n",
    "            if row.has_attr(\"href\"):\n",
    "                next_button_link = ['https://www.immoabita.be'+row['href']]\n",
    "        if i<19:\n",
    "            driver.get(next_button_link[0])\n",
    "   \n",
    "    return prop_links  \n",
    "\n",
    "#Parse html page\n",
    "def get_html_data(url, driver):\n",
    "    driver.get(url)\n",
    "    time.sleep(np.random.lognormal(0,1))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "#Flatten the url list\n",
    "def flatten_list(prop_links):\n",
    "    prop_links_flat=[]\n",
    "    for sublist in prop_links:\n",
    "        for item in sublist:\n",
    "            prop_links_flat.append(item)\n",
    "    return prop_links_flat\n",
    "\n",
    "#Get Property Id\n",
    "def get_propid(url):\n",
    "    try:\n",
    "        prop_id = url.split('/')[-1]\n",
    "        dfProp = pd.DataFrame([[\"Id\",prop_id]], columns = ['Property','Value'])\n",
    "        return dfProp\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "#Get Property Locality\n",
    "def get_locality(soup):\n",
    "    try:\n",
    "        locality = soup.find(\"span\",class_=\"block-xs\").get_text()\n",
    "        dfLoc = pd.DataFrame([[\"Locality\",locality]], columns = ['Property','Value'])\n",
    "        return dfLoc\n",
    "    except:\n",
    "        return 'None'\n",
    "\n",
    "#Get property price\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find(\"span\",class_=\"text-nowrap\").get_text()\n",
    "        dfPrice = pd.DataFrame([[\"Price\",price]], columns = ['Property','Value'])\n",
    "        return dfPrice\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "#Get other attributes from html table\n",
    "def get_others(url):\n",
    "    try:\n",
    "        r = requests.get(url,verify=False)\n",
    "        df_list = pd.read_html(r.text) # this parses all the tables in webpages to a list\n",
    "        df_T1 = pd.DataFrame()\n",
    "        for i in df_list:\n",
    "            df_T2 = pd.DataFrame()\n",
    "            df_T2 = i\n",
    "            df_T1 = df_T1.append(df_T2)\n",
    "        dfOthers = df_T1.rename(columns = {0:'Property',1:'Value'},inplace = False)\n",
    "        return dfOthers\n",
    "    except:\n",
    "        return 'None'\n",
    "\n",
    "#Prepare final data\n",
    "def get_property_data(driver,prop_links):\n",
    "    PropData = pd.DataFrame()\n",
    "    for link in prop_links:\n",
    "        soup = get_html_data(link,driver)\n",
    "        propid = get_propid(link)\n",
    "        locality = get_locality(soup)\n",
    "        price = get_price(soup)\n",
    "        others = get_others(link)\n",
    "        Finaldf = pd.DataFrame()\n",
    "        Finaldf = pd.concat([propid,locality,price,others])\n",
    "        Finaldf = Finaldf.transpose()\n",
    "        Finaldf = Finaldf.reset_index(drop=True)\n",
    "        header = Finaldf.iloc[0]\n",
    "        Finaldf = Finaldf[1:]\n",
    "        Finaldf.columns = header\n",
    "       \n",
    "        Data = pd.DataFrame(columns = ['Property Id','Locality','Type of Property','Price','Number of Rooms','Area','Terrace','Terrace Area','Garden','Surface of the land','Swimming Pool'])\n",
    "       \n",
    "        Data['Property Id'] = Finaldf['Id']\n",
    "        Data['Locality'] = Finaldf['Locality']\n",
    "        Data['Type of Property'] = Finaldf['Category']\n",
    "        Data['Price'] = Finaldf['Price']\n",
    "        Data['Number of Rooms'] = Finaldf['Number of bedrooms']\n",
    "        try:\n",
    "            Data['Area'] = Finaldf['Habitable surface']\n",
    "        except:\n",
    "            Data['Area'] = np.nan\n",
    "        try:\n",
    "            Data['Terrace'] = Finaldf['Terrace']\n",
    "        except:\n",
    "            Data['Terrace'] = np.nan\n",
    "        try:\n",
    "            Data['Terrace Area'] = Finaldf['Terrace 1 (surface)']\n",
    "        except:\n",
    "            Data['Terrace Area'] = np.nan\n",
    "        try:\n",
    "            Data['Garden'] = Finaldf['Garden']\n",
    "        except:\n",
    "            Data['Garden'] = np.nan\n",
    "        try:\n",
    "            Data['Surface of the land'] = Finaldf['Ground surface']\n",
    "        except:\n",
    "            Data['Surface of the land'] = np.nan\n",
    "        try:\n",
    "            Data['Swimming Pool'] = Finaldf['Pool']\n",
    "        except:\n",
    "            Data['Swimming Pool'] = np.nan\n",
    "            \n",
    "   \n",
    "        PropData = PropData.append(Data)\n",
    "       \n",
    "    return PropData\n",
    "\n",
    "#Call functions to prepare data\n",
    "prop_links = get_property_links(url, driver, pages=3)\n",
    "\n",
    "prop_data = get_property_data(driver,prop_links)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "#Write data to csv\n",
    "prop_data.to_csv('ImmoWeb_Property_Sale_Data.csv', index = False, encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
